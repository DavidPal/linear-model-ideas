\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage[numbers]{natbib}

\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\subjectto}{subject\ to}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\K}{\mathcal{K}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}

\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle} % inner product or bilinear map

\begin{document}

\title{Linear model ideas}
\author{David P\'al}
\date{February 3, 2026}
\maketitle

\begin{abstract}
This document is a series of ideas for improving linear models.
\end{abstract}

\section{Convex shape constraints}

Suppose we have observations $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$.  We
assume that $x_t \in \R^d$ and $y_t \in \R$ for all $t=1,2,\dots,T$. We would
like to fit a function $f:\R^d \to \R$ such that
$$
y_t \approx f(x_t) \qquad \text{for all $t=1,2,\dots,T$}.
$$
Of course, this can be done with many different methods, ranging from linear
regression, and decision trees to neural networks.

Suppose that $f:\R^d \to \R$ has to be a convex function. Of course, a linear
or an affine functions is convex, and it can be found by doing least squares
linear regression. A more general function is of the form
$$
f(x) = a + b^T x + x^T C x
$$
where $a \in \R$, $b \in \R^d$ and $C \in \R^{d \times d}$ is a symmetric
positive semi-definite matrix. The parameters $a,b,C$ can be found by solving
a convex optimization program

\begin{align*}
\minimize_{a,b,C} & \qquad \sum_{t=1}^T \left(y_t - a - b^T x_t - x_t^T C x_t \right)^2 \\
\subjectto & \qquad C \succeq 0
\end{align*}

The objective function and the constraint are both convex in the optimization
variables $a,b,C$. To make it obvious that objective is convex in $a,b,C$, we
can write it as
$$
\sum_{i=1}^n \left(y_i - a - b^T x_i - \ip{x_t x_t^T}{C}_F \right)^2
$$
where $\ip{x_i x_i^T}{C}_F$ is the Frobenius inner product between the matrix
$x_t x_t^T$ and the matrix $C$. To see that note that
$$
x_t^T C x_t = \sum_{i=1}^d \sum_{j=1}^d x_{t,i} x_{t,j} C_{i,j} = \ip{x_t x_t^T}{C}_F \: .
$$

With this view in mind, note that the optimization problem is the linear least
squares problem with an additional constraint on the parameters.

\section{Heterogenous treatment effect}

Suppose we have observations $(x_1, y_1, z_1), (x_2, y_2, z_2), \dots, (x_T, y_T, z_T)$.  We
assume that $x_t \in \R^d$, $y_t \in \R$, and $z_t \in \R^k$ for all $t=1,2,\dots,T$. We would
like to fit a function $f:\R^d \times \R^k \to \R$ such that
$$
y_t \approx f(x_t, z_t) \qquad \text{for all $t=1,2,\dots,T$}.
$$
We would like the function $f(x,z)$ to be linear in parameter $x$. A simple function that satisfies
has the form
$$
f(x,z) = a + b^Tx + c^Tz + z^T D x \: .
$$
where $a \in \R$, $b \in \R^d$, $c \in \R^k$, $D \in \R^{k \times d}$ is a matrix. The function can be
equivalently written as
$$
f(x,z) = a + b^Tx + c^Tz + \ip{z x^T}{D}_F \: .
$$
where $\ip{z x^T}{D}_F$ is the Frobenius inner product between two $k \times d$ matrices, $z x^T$
and $D$.

With this view in mind, we can find the parameters $a,b,c,D$ by solving the unconstrained
linear least squares optimization problem
$$
\minimize_{a,b,c,D} \qquad \sum_{t=1}^T \left(y_t - a - b^T x_t - c^T z_t - \ip{z_t x_t^T}{D}_F \right)^2 \: .
$$
Note that this problem is equivalent to the standard linear least squares
problem where the feature vector for $t$-th sample is the concatenation of the
vectors $x_t$, $z_t$ and the (vectorized) matrix $z_t x_t^T$.

\section{Ellipsoids}

Let $x_1, x_2, \dots, x_T$ be fixed vectors in $\R^d$. We assume that the
vectors span $\R^d$. Let
$$
y_t = \theta^T x_t + \epsilon_t \qquad \text{for all $t=1,2,\dots,T$}.
$$
where $\epsilon_1, \epsilon_2, \dots, \epsilon_T$ are i.i.d. noise variables
generated from $N(0,\sigma^2)$.

Let $X$ be the $T \times d$ matrix with rows $x_1, x_2, \dots, x_T$. Let $Y =
(y_1, y_2, \dots, y_T) \in \R^T$. Let
$$
\widehat{\theta} = (X^TX)^{-1}X^T Y
$$
be the unregularized least squares estimate of $\theta$.

\subsection{Confidence ellipsoid}

Using the above assumptions we can construct a confidence set for $\theta$.
Formally, it can be shown that with high probability the parameter $\theta$ lies
in the set
$$
C_r = \{ \theta \in \R^d ~:~ (\theta - \widehat{\theta})^T X^T X (\theta - \widehat{\theta})^T \le r \} \: .
$$
The set $C_r$ is an ellipsoid with the center at $\widehat{\theta}$. The shape
of the ellipsoid is specified by the covariance matrix $X^T X$. The scale
parameter $r$ depends on the confidence level and variance of the noise
$\sigma^2$.

\subsection{Where is the model confident?}

It is possible to find a subset of the feature space we are confident that
the least squares model provides an accurate prediction. That is, a set
of feature vectors $x \in \R^d$ such that
$$
|(\widehat{\theta} - \theta)^T x| \le \alpha .
$$
Formally, let
$$
D_{\alpha,\delta} = \{ x \in \R^d ~:~ 8 \sigma^2 x^T (X^TX)^{-1} x \ln(1/\delta) \le \alpha^2 \} \: .
$$
Then, for any $\alpha > 0$ and $\delta \in (0,1)$, with probability at least $1 - \delta$,
$$
|(\widehat{\theta} - \theta)^T x| \le \alpha \: .
$$
The set $D_{\alpha,\delta}$ is an ellipsoid centered at the origin. The shape
of the ellipsoid is specified by the inverse covariance matrix $(X^T X)^{-1}$.
The scale of the ellipsoid depends on the parameters $\alpha$ and $\delta$
and variance of the noise $\sigma^2$.

\nocite{*}
\bibliography{bibliography}
\bibliographystyle{chicago}

\end{document}
